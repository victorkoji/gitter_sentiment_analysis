{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from nltk.tree import Tree\n",
    "import nltk\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('genesis')\n",
    "\n",
    "#Nome da pasta tema\n",
    "pasta_tema = \"BOT\"\n",
    "\n",
    "#Nome do arquivo\n",
    "nome_arquivo = \"steam_chat_bot\"\n",
    "\n",
    "#Caminho do arquivo\n",
    "path_arquivo = f\"./ChatRooms/{pasta_tema}/{nome_arquivo}/{nome_arquivo}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data loaded ....\n0  messages bigrams done ...\nbigrams done ....\n"
     ]
    }
   ],
   "source": [
    "texts = pd.read_csv(f'{path_arquivo}_threads_pre_processado.csv')\n",
    "print(\"data loaded ....\")\n",
    "\n",
    "#O dataset a ser lido precisa ter as colunas abaixo para funcionar.\n",
    "#A coluna clean é referente ao texto pré-processado.\n",
    "#A coluna sent é referente a data e hora da mensagem.\n",
    "#A coluna username é referente ao nome do usuário da mensagem.\n",
    "texts = texts[['id', 'text', 'clean', 'sent', 'username']]\n",
    "texts['sent'] = pd.to_datetime(texts['sent'])\n",
    "texts['chatroom'] = \"\" #Coluna adicionada\n",
    "texts['mention'] = 0\n",
    "texts['ngrams'] = 0\n",
    "texts['involved'] = 0\n",
    "texts['coupling'] = \"\" #Coluna adicionada\n",
    "texts = texts.fillna(\"\")\n",
    "\n",
    "def unique(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "    return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(word_tokenize(' '.join(texts['clean'])))\n",
    "listBigrams = finder.nbest(bigram_measures.pmi, 10000) \n",
    "sorted_listBigrams = np.array(listBigrams)\n",
    "sorted_listBigrams.sort(axis=1)\n",
    "unique_listBigrams = unique(sorted_listBigrams)\n",
    "\n",
    "def identify_threads(texts):\n",
    "\n",
    "    texts['discussionId'] = [[] for _ in range(len(texts))]\n",
    "    index = 0\n",
    "    thread_id = 0\n",
    "    thread_found = True\n",
    "    for i in range(len(texts)):\n",
    "        j = i\n",
    "        starter = texts.iloc[i,3]\n",
    "        ngram_group = list(\"\")\n",
    "        ngram_group += texts.iloc[i,10].split(',')\n",
    "        involvement_group =list(\"\")\n",
    "        involvement_group.append(texts.iloc[i,4])\n",
    "        if thread_found == True:\n",
    "            thread_id += 1\n",
    "            thread_found = False\n",
    "        cutoff = 4\n",
    "        counter = i + cutoff\n",
    "        pairs = {}\n",
    "        coupling_effect = False\n",
    "        while (j < counter) :\n",
    "            if j == len(texts)-1:\n",
    "                break\n",
    "            j = j + 1\n",
    "            last_matched = 0\n",
    "            matching = \"\"\n",
    "            mention = 0\n",
    "            involved = 0\n",
    "            existing = 0\n",
    "            my_list = texts.iloc[j,10].split(',')\n",
    "            if texts.iloc[j,4] in involvement_group: #involement group\n",
    "                involved = 1\n",
    "            elif any(met in texts.iloc[j,1] for met in involvement_group): #mention from involvement group\n",
    "                mention = 1\n",
    "            elif (ngram_group[0] != '' ) & (my_list[0] != ''): # matching Bi-grams\n",
    "                matching = [s for s in ngram_group if any(xs in s for xs in my_list)]\n",
    "            #back and forth user pattern\n",
    "            pattern = texts.iloc[j-1,4]+\",\"+texts.iloc[j,4]\n",
    "            if pairs.get(pattern):\n",
    "                pairs[pattern] = pairs.get(pattern) + 1\n",
    "            else:\n",
    "                pairs[pattern] = 1\n",
    "            user = \"\"\n",
    "            for key,value in pairs.items():\n",
    "                if value >= 3:\n",
    "                    users = key.split(\",\")\n",
    "                    for u in users:\n",
    "                        if not u in involvement_group:\n",
    "                            user = u\n",
    "                            involvement_group.append(u)\n",
    "                            coupling_effect = True\n",
    "            if coupling_effect:\n",
    "                coupling_effect = False\n",
    "                for k in range(j, i, -1): \n",
    "                    if texts.iloc[k,4] == user:\n",
    "                        texts.iloc[k,11].append(thread_id)\n",
    "                        texts.iloc[k,9] = 1\n",
    "            if (mention == 1) | (involved == 1) | (len(matching) > 0):\n",
    "                texts.iloc[j,6] = mention\n",
    "                texts.iloc[j,7] = len(matching)\n",
    "                texts.iloc[j,8] = involved\n",
    "                existing = any(i in texts.iloc[j,11] for i in texts.iloc[i,11])\n",
    "                if (thread_id not in texts.iloc[j,11]) & (existing == 0):\n",
    "                    texts.iloc[j,11].append(thread_id)\n",
    "                    thread_found = True\n",
    "                    last_matched = j\n",
    "                if (thread_id not in texts.iloc[i,11]) & (existing == 0):\n",
    "                    texts.iloc[i,11].append(thread_id)\n",
    "                    last_matched = j\n",
    "                if thread_found == True:\n",
    "                    involvement_group.append(texts.iloc[j,4])\n",
    "                    ngram_group += texts.iloc[j,10].split(',')\n",
    "            if ((j == counter)) & (last_matched != 0):\n",
    "                if (counter - last_matched <= 2) :\n",
    "                    counter += 4\n",
    "                    index+=1\n",
    "    return texts\n",
    "\n",
    "texts['bigrams'] = \"\"\n",
    "for i in range(len(texts)):\n",
    "    flag = False\n",
    "    text = texts.iloc[i,2]\n",
    "    keywords = \"\"\n",
    "    if i % 1000 == 0:\n",
    "        print(i,\" messages bigrams done ...\")\n",
    "    for bigram in unique_listBigrams:\n",
    "        if bigram[0]+\" \"+bigram[1] in text or bigram[1]+\" \"+bigram[0] in text:\n",
    "            if(keywords == \"\"):\n",
    "                keywords = bigram[0]+\" \"+bigram[1] \n",
    "            else:\n",
    "                keywords = keywords + \", \" + bigram[0]+\" \"+bigram[1] \n",
    "    texts.iloc[i,8] =keywords\n",
    "print(\"bigrams done ....\")\n",
    "\n",
    "identify_threads = identify_threads(texts)\n",
    "identify_threads.to_csv(fr\"{path_arquivo}_threads_identificadas.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}